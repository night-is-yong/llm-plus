[2024-02-16 22:31:16,151][inference][WARNING] - `new_tokens` is deprecated. Use `max_new_tokens` and `min_new_tokens` instead. Setting `max_new_tokens` and `min_new_tokens` to `new_tokens`.
[2024-02-16 22:31:16,482][launcher][INFO] - ََAllocating process launcher
[2024-02-16 22:31:16,482][process][INFO] - 	+ Setting multiprocessing start method to spawn.
[2024-02-16 22:31:16,484][process][INFO] - 	+ Launched worker process with PID 279679.
[2024-02-16 22:31:18,126][datasets][INFO] - PyTorch version 2.2.0 available.
[2024-02-16 22:31:18,482][backend][INFO] - َAllocating pytorch backend
[2024-02-16 22:31:18,483][backend][INFO] - 	+ Setting random seed to 42
[2024-02-16 22:31:18,662][pytorch][INFO] - 	+ Using AutoModel class AutoModelForCausalLM
[2024-02-16 22:31:18,662][pytorch][INFO] - 	+ Processing quantization config
[2024-02-16 22:31:18,662][pytorch][INFO] - 	+ Processing AWQ config
[2024-02-16 22:31:18,673][pytorch][INFO] - 	+ Creating backend temporary directory
[2024-02-16 22:31:18,674][pytorch][INFO] - 	+ Loading model with no weights
[2024-02-16 22:31:18,674][pytorch][INFO] - 	+ Creating no weights model state_dict
[2024-02-16 22:31:18,674][pytorch][INFO] - 	+ Creating no weights model directory
[2024-02-16 22:31:18,674][pytorch][INFO] - 	+ Saving no weights model pretrained config
[2024-02-16 22:31:18,675][pytorch][INFO] - 	+ Saving no weights model state_dict
[2024-02-16 22:31:18,675][pytorch][INFO] - 	+ Loading no weights model
[2024-02-16 22:31:18,675][pytorch][INFO] - 	+ Loading quantized model
[2024-02-16 22:31:20,246][pytorch][INFO] - 	+ Randomizing model weights
[2024-02-16 22:31:20,248][pytorch][INFO] - 	+ Tying model weights
[2024-02-16 22:31:20,249][pytorch][INFO] - 	+ Turning on model's eval mode
[2024-02-16 22:31:20,330][benchmark][INFO] - Allocating inference benchmark
[2024-02-16 22:31:20,330][inference][INFO] - 	+ Creating input generator
[2024-02-16 22:31:20,330][input][INFO] - 	+ Using text-generation task generator
[2024-02-16 22:31:20,330][inference][INFO] - 	+ Generating and preparing Text Generation input
[2024-02-16 22:31:20,331][pytorch][INFO] - 	+ Moving inputs tensors to device cuda
[2024-02-16 22:31:20,331][pytorch][INFO] - 	+ Moving inputs tensors to device cuda
[2024-02-16 22:31:20,331][inference][INFO] - 	+ Updating Text Generation kwargs with default values
[2024-02-16 22:31:20,331][inference][INFO] - 	+ Initializing Text Generation report
[2024-02-16 22:31:20,331][inference][INFO] - 	+ Preparing backend for Inference
[2024-02-16 22:31:20,331][inference][INFO] - 	+ Warming up backend for Inference
[2024-02-16 22:31:21,673][inference][INFO] - 	+ Creating inference memory tracker
[2024-02-16 22:31:21,673][memory][INFO] - 	+ Tracking VRAM memory of CUDA devices: [0]
[2024-02-16 22:31:21,673][memory][INFO] - 	+ Tracking Pytorch memory of Pytorch CUDA devices: [0]
[2024-02-16 22:31:21,673][inference][INFO] - 	+ Running memory tracking
[2024-02-16 22:31:39,929][report][INFO] - 	+ prefill.memory.max_vram_used(MB): 8627.000000 (MB)
[2024-02-16 22:31:39,929][report][INFO] - 	+ prefill.memory.max_memory_reserved(MB): 7484.000000 (MB)
[2024-02-16 22:31:39,929][report][INFO] - 	+ prefill.memory.max_memory_allocated(MB): 7192.000000 (MB)
[2024-02-16 22:31:39,929][report][INFO] - 	+ decode.memory.max_vram_used(MB): 10698.000000 (MB)
[2024-02-16 22:31:39,929][report][INFO] - 	+ decode.memory.max_memory_reserved(MB): 9084.000000 (MB)
[2024-02-16 22:31:39,929][report][INFO] - 	+ decode.memory.max_memory_allocated(MB): 8626.000000 (MB)
[2024-02-16 22:31:39,929][inference][INFO] - 	+ Creating inference latency tracker
[2024-02-16 22:31:39,929][latency][INFO] - 	+ Tracking Pytorch CUDA latency
[2024-02-16 22:31:39,929][inference][INFO] - 	+ Running latency tracking
[2024-02-16 22:32:04,724][report][INFO] - 	+ prefill.latency.mean(s): 0.090276 (s)
[2024-02-16 22:32:04,724][report][INFO] - 	+ prefill.latency.stdev(s): 0.003043 (s)
[2024-02-16 22:32:04,724][report][INFO] - 	+ prefill.throughput.mean[tokens/s]: 5677.358364 (tokens/s)
[2024-02-16 22:32:04,724][report][INFO] - 	+ prefill.throughput.stdev[tokens/s]: 175.586025 (tokens/s)
[2024-02-16 22:32:04,724][report][INFO] - 	+ decode.latency.mean(s): 14.677237 (s)
[2024-02-16 22:32:04,724][report][INFO] - 	+ decode.latency.stdev(s): 0.000000 (s)
[2024-02-16 22:32:04,724][report][INFO] - 	+ decode.throughput.mean[tokens/s]: 68.132717 (tokens/s)
[2024-02-16 22:32:04,724][report][INFO] - 	+ decode.throughput.stdev[tokens/s]: 0.000000 (tokens/s)
[2024-02-16 22:32:04,724][backend][INFO] - Cleaning pytorch backend
[2024-02-16 22:32:04,811][pytorch][INFO] - 	+ Cleaning backend temporary directory
