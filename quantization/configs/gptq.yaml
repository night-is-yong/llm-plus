defaults:
  - _base_
  - _self_

experiment_name: gptq-batch_size(${benchmark.input_shapes.batch_size})-sequence_length(${benchmark.input_shapes.sequence_length})-new_tokens(${benchmark.new_tokens})

backend:
  model: /home/chuan/models/qwen/Qwen1___5-7B-Chat-GPTQ-Int4
  quantization_scheme: gptq
  quantization_config:
    bits: 4
  torch_dtype: float16